{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV VERSION (should be 3.1.0 or later, with nonfree modules installed!): 3.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import visual_bow as bow\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.externals import joblib\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9106 total negative imgs to choose from\n",
      "[('101_ObjectCategories/rooster/image_0014.jpg', False), ('101_ObjectCategories/rooster/image_0023.jpg', False), ('101_ObjectCategories/rooster/image_0040.jpg', False), ('101_ObjectCategories/rooster/image_0013.jpg', False), ('101_ObjectCategories/rooster/image_0038.jpg', False)]\n"
     ]
    }
   ],
   "source": [
    "# Get all possible negative images and label them False\n",
    "positive_folder='panda'\n",
    "all_negs = [(path, False) for path in bow.neg_img_cal101(positive_folder)]\n",
    "print '%i total negative imgs to choose from' % len(all_negs)\n",
    "print all_negs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 positive images\n",
      "[('panda_rip/image_0014.jpg', True), ('panda_rip/75.JPEG', True), ('panda_rip/345.JPEG', True), ('panda_rip/30.JPEG', True), ('panda_rip/106.JPEG', True)]\n"
     ]
    }
   ],
   "source": [
    "# Get all the positive images you have (in the panda_rip folder) and label them True\n",
    "positive_imgs = [(path, True) for path in glob.glob('panda_rip/*')]\n",
    "print '%i positive images' % len(positive_imgs)\n",
    "print positive_imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350 total images (1:1 positive:negative)\n",
      "[('panda_rip/825.JPEG', True), ('101_ObjectCategories/Faces/image_0013.jpg', False), ('101_ObjectCategories/BACKGROUND_Google/image_0090.jpg', False), ('101_ObjectCategories/octopus/image_0009.jpg', False), ('101_ObjectCategories/joshua_tree/image_0001.jpg', False)]\n"
     ]
    }
   ],
   "source": [
    "# take N random negative images, where N is no of positive images\n",
    "# then concatenate N pos + N neg and shuffle.\n",
    "chosen_negs = random.sample(all_negs, len(positive_imgs))\n",
    "imgs = chosen_negs + positive_imgs\n",
    "\n",
    "np.random.shuffle(imgs)\n",
    "\n",
    "print '%i total images (1:1 positive:negative)' % len(imgs)\n",
    "print imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating SIFT keypoints for 1350 images\n",
      "Train-test-val split: 946 training rows, 202 test rows, 202 validation rows\n",
      "390833 descriptors before clustering\n",
      "Using clustering model MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10,\n",
      "        n_clusters=250, n_init=3, random_state=None,\n",
      "        reassignment_ratio=0.01, tol=0.0, verbose=0)...\n",
      "Clustering on training set to get codebook of 250 words\n",
      "done clustering\n",
      "CPU times: user 10min 41s, sys: 22.8 s, total: 11min 3s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "K_CLUSTERS = 250\n",
    "\n",
    "# MiniBatchKMeans annoyingly throws tons of deprecation warnings that fill up the notebook. Ignore them.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val, cluster_model = bow.gen_bow_features(\n",
    "    labeled_img_paths=imgs, \n",
    "    percent_test=0.15,\n",
    "    percent_val=0.15,\n",
    "    cluster_model=MiniBatchKMeans(n_clusters=K_CLUSTERS)\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to pickle the SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for obj, obj_name in zip( [X_train, X_test, X_val, y_train, y_test, y_val], \n",
    "                         ['X_train', 'X_test', 'X_val', 'y_train', 'y_test', 'y_val'] ):\n",
    "    joblib.dump(obj, 'pickles/feature_data/%s.pickle' % obj_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to UNpickle the SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for obj_name in ['X_train', 'X_test', 'X_val', 'y_train', 'y_test', 'y_val']:\n",
    "#     exec(\"{obj_name} = joblib.load('pickles/feature_data/{obj_name}.pickle')\".format(obj_name=obj_name))\n",
    "#     exec(\"print obj_name, len({0})\".format(obj_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score (mean accuracy): 0.918604651163\n",
      "test score (mean accuracy): 0.861386138614\n",
      "SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "CPU times: user 1.49 s, sys: 82.5 ms, total: 1.58 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# c_vals = [0.0001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "c_vals = [0.1, 1, 5, 10]\n",
    "# c_vals = [1]\n",
    "\n",
    "gamma_vals = [0.5, 0.1, 0.01, 0.0001, 0.00001]\n",
    "# gamma_vals = [0.5, 0.1]\n",
    "# gamma_vals = [0.1]\n",
    "\n",
    "param_grid = [\n",
    "  {'C': c_vals, 'kernel': ['linear']},\n",
    "  {'C': c_vals, 'gamma': gamma_vals, 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = GridSearchCV(SVC(), param_grid, n_jobs=-1)\n",
    "svc.fit(X_train, y_train)\n",
    "print 'train score (mean accuracy):', svc.score(X_train, y_train)\n",
    "print 'test score (mean accuracy):', svc.score(X_test, y_test)\n",
    "print svc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have our estimator, this is how it could classify random pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101_ObjectCategories/BACKGROUND_Google/image_0021.jpg ['True']\n",
      "101_ObjectCategories/gerenuk/image_0015.jpg ['False']\n",
      "101_ObjectCategories/Faces_easy/image_0098.jpg ['False']\n",
      "101_ObjectCategories/Faces/image_0215.jpg ['False']\n",
      "101_ObjectCategories/octopus/image_0005.jpg ['False']\n",
      "101_ObjectCategories/Motorbikes/image_0369.jpg ['False']\n",
      "101_ObjectCategories/starfish/image_0041.jpg ['False']\n",
      "101_ObjectCategories/BACKGROUND_Google/image_0161.jpg ['False']\n",
      "101_ObjectCategories/Motorbikes/image_0533.jpg ['False']\n",
      "101_ObjectCategories/airplanes/image_0006.jpg ['False']\n"
     ]
    }
   ],
   "source": [
    "for img_path, label in random.sample(all_negs, 10):\n",
    "    print img_path, svc.predict(bow.img_to_vect(img_path, cluster_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to pickle the best SVC classifier & kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickles/cluster_model/cluster_model.pickle',\n",
       " 'pickles/cluster_model/cluster_model.pickle_01.npy',\n",
       " 'pickles/cluster_model/cluster_model.pickle_02.npy',\n",
       " 'pickles/cluster_model/cluster_model.pickle_03.npy']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svc.best_estimator_, 'pickles/svc/svc.pickle')\n",
    "joblib.dump(cluster_model, 'pickles/cluster_model/cluster_model.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try AdaBoost, it's a common choice for SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score (mean accuracy): 0.998942917548\n",
      "test score (mean accuracy): 0.831683168317\n",
      "CPU times: user 2.66 s, sys: 3.91 ms, total: 2.66 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MAX_ESTIMATORS = 200\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=MAX_ESTIMATORS)\n",
    "ada.fit(X_train, y_train)\n",
    "print 'train score (mean accuracy):', ada.score(X_train, y_train)\n",
    "print 'test score (mean accuracy):', ada.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to pickle the AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked adaboost\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(ada, 'pickles/ada/ada.pickle');\n",
    "print 'picked adaboost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Separate out the clustering from the feature generation. They should be 2 different functions, the clustering should take the SIFT **training** data as an argument. It has labels already, right? Then you can save the SIFT data before clustering. Finally, you can do a grid search across K_CLUSTERS.\n",
    "\n",
    "* Also it would be cool to graph the above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
